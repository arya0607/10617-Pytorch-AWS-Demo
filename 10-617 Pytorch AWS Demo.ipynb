{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "LTOxErZWluNP"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import math"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1OqXiibkjd8b"
      },
      "source": [
        "## Tensors\n",
        "\n",
        "Tensors are the basic units of storage and computation in PyTorch. They are very similar to NumPy arrays, and follow the same syntax for indexing and for most arithmetic operations.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VQ6v6uTCjcqm",
        "outputId": "2b9083a3-d853-4c11-b4da-a78bb4e0b324"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([1., 2., 3., 4., 5.]) torch.Size([5])\n"
          ]
        }
      ],
      "source": [
        "# Basic tensor operations\n",
        "x_tensor = torch.tensor([1, 2, 3, 4, 5])\n",
        "y_tensor = torch.ones((2, 5))\n",
        "prod = x_tensor * y_tensor[1,:]\n",
        "print(prod, prod.shape) # Use .shape to view shape just as in NumPy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "t1POkPN7oHDl"
      },
      "outputs": [],
      "source": [
        "# Easy conversion between PyTorch and Numpy\n",
        "x_numpy = np.array([1, 2, 3, 4, 5])\n",
        "assert (x_tensor.numpy() == x_numpy).all()\n",
        "assert (x_tensor == torch.from_numpy(x_numpy)).all()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s0ovXbiFmMA_"
      },
      "source": [
        "What makes tensors **different** from NumPy arrays is:\n",
        "\n",
        "1. They support GPU acceleration.\n",
        "2. They contain additional information (accessed as attributes, which we will see later) which facilitate the easy building and training of neural networks, via **computation graphs** and **gradients**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "At9t1bdM7vp_"
      },
      "source": [
        "## A motivating example\n",
        "\n",
        "To see the advantages of PyTorch in action, let's first take a look at how one might train a simple model using only NumPy:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jJo7qe3_77c_",
        "outputId": "17210601-421b-4eef-c045-0156b5a346f6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0: loss 354519.8758933898\n",
            "Epoch 100: loss 794.7239960893968\n",
            "Epoch 200: loss 562.0635603527484\n",
            "Epoch 300: loss 398.40542930583405\n",
            "Epoch 400: loss 283.2420958591989\n",
            "Epoch 500: loss 202.17610814954816\n",
            "Epoch 600: loss 145.0936432014566\n",
            "Epoch 700: loss 104.88693688840125\n",
            "Epoch 800: loss 76.55876613193665\n",
            "Epoch 900: loss 56.59439224594736\n",
            "Epoch 1000: loss 42.52085684691431\n",
            "Epoch 1100: loss 32.59759069446356\n",
            "Epoch 1200: loss 25.599108283326906\n",
            "Epoch 1300: loss 20.662312917805394\n",
            "Epoch 1400: loss 17.179157079491223\n",
            "Epoch 1500: loss 14.721156255060043\n",
            "Epoch 1600: loss 12.986283710371358\n",
            "Epoch 1700: loss 11.761597245134212\n",
            "Epoch 1800: loss 10.89692866423157\n",
            "Epoch 1900: loss 10.286355483284476\n",
            "Result: y = 0.03376121343570352 + 0.8519525810646256 x + -0.005824371106675349 x^2 + -0.09264930046634844 x^3\n"
          ]
        }
      ],
      "source": [
        "# Create 2000 random input and output data points\n",
        "x = np.linspace(-math.pi, math.pi, 2000)\n",
        "y = np.sin(x)\n",
        "\n",
        "# Step 0 - Initialization: Initialize model parameters (randomly, in this example)\n",
        "a = np.random.randn()\n",
        "b = np.random.randn()\n",
        "c = np.random.randn()\n",
        "d = np.random.randn()\n",
        "\n",
        "learning_rate = 1e-6\n",
        "for t in range(2000):\n",
        "    # Step 1 - Forward Pass: compute predicted y according to the following model:\n",
        "    # y = a + b x + c x^2 + d x^3\n",
        "    y_pred = a + b * x + c * x ** 2 + d * x ** 3\n",
        "\n",
        "    # Step 2 - Compute Loss (MSE in this example)\n",
        "    loss = np.square(y_pred - y).sum() # Sum across 2000 training points\n",
        "    if t % 100 == 0:\n",
        "        print(f'Epoch {t}: loss {loss}')\n",
        "\n",
        "    # Step 3 - Differentiate: Use backprop (chain rule) to compute gradients\n",
        "    # of the loss function with respect to a, b, c, d\n",
        "    grad_y_pred = 2.0 * (y_pred - y)\n",
        "    grad_a = grad_y_pred.sum() # Sum across 2000 training points\n",
        "    grad_b = (grad_y_pred * x).sum()\n",
        "    grad_c = (grad_y_pred * x ** 2).sum()\n",
        "    grad_d = (grad_y_pred * x ** 3).sum()\n",
        "\n",
        "    # Step 4 - Optimize: Update weights (vanilla GD in this example)\n",
        "    a -= learning_rate * grad_a\n",
        "    b -= learning_rate * grad_b\n",
        "    c -= learning_rate * grad_c\n",
        "    d -= learning_rate * grad_d\n",
        "\n",
        "print(f'Result: y = {a} + {b} x + {c} x^2 + {d} x^3')\n",
        "\n",
        "# From: https://pytorch.org/tutorials/beginner/pytorch_with_examples.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BIfNG71OG9Jq"
      },
      "source": [
        "Each step involved in training a model is coded very explicitly in this example:\n",
        "0. Initialization: Initialize model parameters (i.e. weights)\n",
        "1. Forward Pass: Compute predictions\n",
        "2. Compute Loss\n",
        "3. Differentiate: Compute gradients via backpropagation (chain rule)\n",
        "4. Optimize: Update weights using gradients\n",
        "\n",
        "Notice how:\n",
        "- Every model parameter is stored as a variable, which contains a weight value to be continuously updated through training\n",
        "- The calculation of the gradient of the loss function with respect to the model parameters is coded explicitly and thus *model dependent*\n",
        "<!-- - The gradient of the loss function needs to be computed with respect to the model parameters explicitly using the chain rule -->\n",
        "- Each model parameter's gradient is used to update the weight value according to a particular update rule which is also explicitly implemented - in this example, standard gradient descent (GD).\n",
        "\n",
        "Steps 0 and 1 might not be so difficult using a basic package such as NumPy; however, one could imagine that steps 2 and 3 would quickly become too burdensome to implement manually for models with more parameters, both because of 1) the large number of parameters and 2) the fact that the explicit implementation of the chain rule and update rule will look different every time depending on the the model structure and optimization algorithm.\n",
        "\n",
        "This is where PyTorch's **automatic differentiation** engine, **Autograd**, is useful. As the name implies, PyTorch can take care of computing gradients and applying optimization update rules automatically under the hood, such that the code we write to invoke these two operaations will look (very nearly) the same across all models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IcRm_01MryEd"
      },
      "source": [
        "## Computation Graphs and Autograd\n",
        "\n",
        "<!-- Old paragraphs, commented out to preserve in case we want to bring back language -->\n",
        "<!-- Autograd, PyTorch's automatic differentiation engine, is one of the best things that PyTorch has to offer when it comes to training neural networks! Recall that in order to compute gradients for weights in a neural network, you have to backpropagate errors at each node. This is computationally intensive even for a one or two layers and would be a nightmare to implement manually for very deep networks. -->\n",
        "<!-- PyTorch tensors can keep track of a *computation graph* whenever operations are performed on/between tensors. Autograd performs backprop on this computation graph, so all the steps to compute the gradient are abstracted away. To perform a gradient step update, you simply have to call built-in PyTorch functions that computes the derivative of the loss and (if you're using gradient descent) takes a step toward a gradient of zero. -->\n",
        "\n",
        "To implement automatic differentiation, PyTorch keeps track of a **computation graph** which logs every operation used to construct a certain output or mathematical expression (more on computation graphs in the [Appendix](#Appendix)). Computation graphs are created automatically whenever mathematical operations are performed using tensors as inputs. Using this graph, **Autograd** can perform backpropagation by repeated applications of the chain rule, and all of these differentiation steps are abstracted away from the user. To perform weight updates, we simply call built-in PyTorch optimizer functions which implement different update rules using the gradients computed during backpropagation.\n",
        "\n",
        "As we work through the following example, keep in mind that computation graphs in PyTorch are *implicitly* constructed when one builds mathematical expressions. That is, one will never have to write code expressly for the purpose of building such a graph; rather, the computation graph will be assembled automatically using whichever Tensors or operations are appropriately \"earmarked\" for tracking (see `requires_grad` below).\n",
        "\n",
        "## Demo #1\n",
        "\n",
        "Let's start with a simple example of autograd.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-oQ3VCa-wEnL",
        "outputId": "8910643a-c7be-464e-a747-389d5ff46546"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "x: tensor([2.])\n",
            "y: tensor([-1.])\n",
            "z: tensor([4.])\n"
          ]
        }
      ],
      "source": [
        "# Create tensors\n",
        "x = torch.Tensor([2])\n",
        "y = torch.Tensor([-1])\n",
        "z = torch.Tensor([4])\n",
        "\n",
        "print('x:', x)\n",
        "print('y:', y)\n",
        "print('z:', z)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xoGuhefUwfz4"
      },
      "source": [
        "Tensors have a ```requires_grad``` field. When ```requires_grad == True``` for a certain tensor, any subsequent operations that this tensor is an input to will be incorporated into a computation graph. Autograd will then be able to automatically compute partial derivatives *with respect to this tensor*. (Derivative of what function? More on that [later](#step5).)\n",
        "\n",
        "For any tensors which are the outputs of operations tracked in the computation graph, the ```grad_fn``` attribute will indicate the operation that created the tensor, represented as a `Function` object. (It is these `Function`s which encode the computation graph DAG [Directed Acyclic Graph].)\n",
        "\n",
        "* If some starting tensors have requires_grad = True, then every tensor resulting from operations of those starting tensors will also have requires_grad = True.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tNy9_NsXwXPp",
        "outputId": "cc733dce-c124-4a80-9556-95ec1c8e2a29"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "x and y requires_grad: False False\n",
            "a grad_fn: None\n"
          ]
        }
      ],
      "source": [
        "# By default, user created Tensors have ``requires_grad=False``\n",
        "print('x and y requires_grad:', x.requires_grad, y.requires_grad)\n",
        "\n",
        "a = (x + y) * z\n",
        "\n",
        "# So you can't backprop through z\n",
        "print('a grad_fn:', a.grad_fn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xnb3o2ayxFyX",
        "outputId": "2762268e-03b9-4e5b-ef6f-5bf0704c83a3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([4.], grad_fn=<PowBackward0>)\n"
          ]
        }
      ],
      "source": [
        "# ``.requires_grad_( ... )`` changes an existing Tensor's ``requires_grad`` flag in-place.\n",
        "# The input flag defaults to ``True`` if not given.\n",
        "x = x.requires_grad_()\n",
        "y = y.requires_grad_()\n",
        "z = z.requires_grad_()\n",
        "\n",
        "# a contains enough information to compute gradients, as we saw above\n",
        "a = (x + y) * z\n",
        "b = 0.5 * a\n",
        "c = b**2\n",
        "print(c)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MF8igZvpJSME",
        "outputId": "814e5510-4e52-4146-fd01-cdd5d17d420f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "a grad_fn: <MulBackward0 object at 0x7f62e1fdbdc0>\n",
            "a requires_grad: True\n"
          ]
        }
      ],
      "source": [
        "print('a grad_fn:', a.grad_fn)\n",
        "# If any input to an operation has ``requires_grad=True``, so will the output\n",
        "print('a requires_grad:', a.requires_grad)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jWWR8oF4Jh-i"
      },
      "source": [
        "In pytorch, we use the \".backward()\" function to calculate gradients.\n",
        "\n",
        "* When we call \"v.backward()\" on the variable v, this means we are calculating the gradient of v, with respect to all other variables x that were used to compute v.  \n",
        "* In other words, after calling v.backward(), then the variables x have the gradient $\\frac{dv}{dx}$ stored in x.grad."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gOOnjBSoJYMo",
        "outputId": "882b8784-5990-4472-d424-3bb9b748ac27"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "dc/dx:  tensor([8.])\n",
            "dc/dy:  tensor([8.])\n",
            "dc/dz:  tensor([2.])\n"
          ]
        }
      ],
      "source": [
        "# call .backward() on the variable that you want to calculate the gradient of\n",
        "c.backward()\n",
        "\n",
        "print('dc/dx: ', x.grad)\n",
        "print('dc/dy: ', y.grad)\n",
        "print('dc/dz: ', z.grad)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xe2dUCuuTpo5"
      },
      "source": [
        "Now that we've been introduced to `requires_grad`, `grad_fn` and the computation graph....\n",
        "\n",
        "#### Question: Where should I actually set requires_grad to make everything work?\n",
        "#### Short Answer: In any tensors which represent model parameters or weights.\n",
        "\n",
        "Long Answer:\n",
        "\n",
        "- If you are initializing a parameter(s)/weight(s) by creating an individual tensor(s) manually, you have to set its `requires_grad = True` argument.\n",
        "- If you are initializing a parameter(s)/weight(s) by using built-in PyTorch methods from packages such as nn.Layer and nn.Parameter, the internally initialized tensors should already have `requires_grad` set to `True`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zIFDbUwUrvCK"
      },
      "source": [
        "## GPU Acceleration\n",
        "\n",
        "GPUs are built for parallel processing — they can handle thousands of operations at the same time — which makes them great for computationally-intensive neural nets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "7QdobpsapbDV"
      },
      "outputs": [],
      "source": [
        "# To run tensor operations on GPU in Colab, go to Edit -> Notebook settings -> and select GPU as the hardware accelerator.\n",
        "# If GPU is enabled, the following assertion should pass.\n",
        "assert torch.cuda.is_available()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "djVoc6o0pqIn",
        "outputId": "8791d1ff-250a-4b1d-95ca-aea96d78290a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[0.4832, 0.0665, 0.0197, 0.2763, 0.1252],\n",
              "        [0.1414, 0.0459, 0.0023, 0.0138, 0.3642],\n",
              "        [0.0625, 0.0554, 0.1053, 0.2693, 0.3180],\n",
              "        [0.2612, 0.0769, 0.2709, 0.1622, 0.0142],\n",
              "        [0.0058, 0.0308, 0.0354, 0.6303, 0.2456]], device='cuda:0')"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# When creating a tensor, set its device to use CUDA (a GPU parallel computing platform), as shown below.\n",
        "# What this means is that we are telling PyTorch to physically initialize the relevant\n",
        "# variable in GPU memory.\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "x = torch.rand(5, 5, device=device)\n",
        "y = torch.rand(5, 5, device=device)\n",
        "x * y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9jt4jB6bKNcH"
      },
      "source": [
        "# Training a Neural Network on CIFAR-10\n",
        "\n",
        "In this part, we will train a neural network to perform multi-class classifications on the [CIFAR-10 dataset](https://www.cs.toronto.edu/~kriz/cifar.html). This dataset consists of 32x32 images belonging to 10 classes.\n",
        "\n",
        "In particular, we will train a fully-connected neural network with 1 hidden layer and a ReLU activation function.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UwVMmH7gvpEv"
      },
      "source": [
        "## Step 1: Load the Dataset\n",
        "\n",
        "We use helper functions from the torchvision.datasets library to load the CIFAR-10 dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101,
          "referenced_widgets": [
            "21a599bdc17a4eab9d8a6766aaabb074",
            "7f0005cccc024fb78835675acc781782",
            "0cb6afa0eb5f47309a71778b6fed41a4",
            "67b08f0df2e94faaa144bad0e928499f",
            "603e8832de9041a7a2653de312ab70c1",
            "727a67a3733748eaa15351e5d91996d3",
            "9613c3fe94f646e39f40b3bd0fc92dab",
            "6e8713e1ac4b4a47ac288b07416f773a",
            "b03d9640d8234968b333461d163aafe5",
            "e9719121068246c4b57aaa7e73fec47e",
            "e682e46a6b1d478caaef1660e0affcdc"
          ]
        },
        "id": "yhFQv0dAvu7q",
        "outputId": "f4712846-c26a-421f-c05d-90fa3b1e60b5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./part_2_data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "21a599bdc17a4eab9d8a6766aaabb074",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/170498071 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./part_2_data/cifar-10-python.tar.gz to ./part_2_data\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "## Download CIFAR-10 dataset\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "# A transformation applied to the images of the dataset\n",
        "# Update the data to tensors and normalize their values by setting the mean and std dev\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./part_2_data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./part_2_data', train=False,\n",
        "                                       download=True, transform=transform)\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat',\n",
        "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OtqfAMCQv47f"
      },
      "outputs": [],
      "source": [
        "ex_img, ex_label = trainset[30]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uiDWy6FdxA-B"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def display_img(im):\n",
        "  t1 = np.swapaxes(im, 0, 2)\n",
        "  t2 = np.swapaxes(t1, 0, 1)\n",
        "  plt.imshow(t2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 301
        },
        "id": "iGUqEF-sv_hw",
        "outputId": "0de7ec1e-7e18-40bb-9ab8-2a3da544ab57"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "plane\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "print(classes[ex_label])\n",
        "display_img(ex_img)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hdONEpz_5XMo"
      },
      "source": [
        "### torch DataLoaders\n",
        "\n",
        "[DataLoaders](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html) support iterating through a pytorch dataset.  They are initialized with two relevant arguments:\n",
        "* `batch_size`: the number of data-points that the dataloader outputs during each iteration\n",
        "* `shuffle`: if true, the dataloader returns the dataset in a different order each time that you iterate through it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DX1ftuae4X5K"
      },
      "outputs": [],
      "source": [
        "# Define dataloaders\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
        "                                          shuffle=True)\n",
        "\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
        "                                         shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "id": "AIIIRKvi52gQ",
        "outputId": "e7719912-8a54-4bbc-9585-a1a0a14e6d7b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "for x, y in trainloader:\n",
        "  display_img(x[0])\n",
        "  break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G4m2OCjwxVet"
      },
      "source": [
        "## 2. Define the model\n",
        "\n",
        "We use the torch.nn package to define our model as a sequence of layers.  The pytorch library has already [defined their own classes](https://pytorch.org/tutorials/beginner/basics/buildmodel_tutorial.html) for most types of layers and activation functions.  You can use these torch classes as \"building blocks\" to specify the architecture of your own neural network.\n",
        "\n",
        "Defining a model typically involves implementing two functions:\n",
        "* init(self, -): initialize python classes corresponding to each layer (and activation function) for the network.\n",
        "* forward(self, x): specifies the order in which each layer is applied to the input to calculate the forward pass on input x.\n",
        "\n",
        "Note that you *do not* need to implement your own backward function! - torch automatically constructs a computational graph (which implicitly specifies how to compute the backward pass) from your forward pass code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RDAhL2Svxyp3"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xRHMK9qny9as",
        "outputId": "5a692d9e-09ed-4157-ed2e-abeca8a1f1b6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "image shape:  torch.Size([3, 32, 32])\n",
            "input dim:  3072\n"
          ]
        }
      ],
      "source": [
        "print('image shape: ', ex_img.shape)\n",
        "\n",
        "input_dim = ex_img.shape[0] * ex_img.shape[1] * ex_img.shape[2]\n",
        "print('input dim: ', input_dim)\n",
        "\n",
        "hidden_layer_dim = 100\n",
        "num_classes = len(classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PXuF9oWox3y4"
      },
      "outputs": [],
      "source": [
        "class MyNeuralNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "      super(MyNeuralNetwork, self).__init__()\n",
        "\n",
        "      self.hidden_layer = nn.Linear(input_dim, hidden_layer_dim)\n",
        "      self.relu = nn.ReLU()\n",
        "      self.output_layer = nn.Linear(hidden_layer_dim, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "      o1 = self.hidden_layer(x)\n",
        "      o2 = self.relu(o1)\n",
        "      o3 = self.output_layer(o2)\n",
        "      return o3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W-eguGZLz8jn",
        "outputId": "70ae8672-02ea-4f77-a949-5c92f20b95ca"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([3, 32, 32])"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ex_img.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G0_5Laz1zw12"
      },
      "outputs": [],
      "source": [
        "model = MyNeuralNetwork()\n",
        "\n",
        "sm = torch.nn.Softmax(dim = 1)\n",
        "\n",
        "b_pred = model.forward(ex_img.reshape(1, -1))\n",
        "yhat_pred = sm(b_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "92b5UpDz0NE2",
        "outputId": "54991c19-b519-4227-e23f-d3ceed4f3d10"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[0.1092, 0.1044, 0.0983, 0.0809, 0.1165, 0.0862, 0.1049, 0.0948, 0.0989,\n",
              "         0.1060]], grad_fn=<SoftmaxBackward0>)"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "yhat_pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xukLxueh1WKg",
        "outputId": "34b9eac9-43ec-40e3-f71b-661d1b53e2d2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Parameter containing:\n",
              "tensor([[ 1.7521e-02, -1.2595e-02, -1.3309e-02,  ..., -5.9593e-03,\n",
              "          1.5547e-02,  1.2579e-02],\n",
              "        [ 3.2433e-04,  3.9255e-03, -9.5559e-04,  ..., -1.3419e-02,\n",
              "         -9.6976e-03,  9.3373e-03],\n",
              "        [-1.0370e-02, -1.5703e-02, -9.1657e-03,  ...,  9.4230e-03,\n",
              "         -8.3546e-03, -7.8710e-03],\n",
              "        ...,\n",
              "        [-1.2677e-02,  1.3394e-02, -3.6977e-03,  ...,  1.0627e-02,\n",
              "         -1.2893e-02,  1.0810e-03],\n",
              "        [ 8.1324e-03, -1.0579e-02,  1.6869e-02,  ...,  1.4309e-02,\n",
              "         -8.3908e-03,  6.6035e-03],\n",
              "        [ 1.5013e-02,  1.7690e-02, -1.7704e-02,  ..., -1.1905e-02,\n",
              "          8.2501e-04, -9.2577e-05]], requires_grad=True)"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.hidden_layer.weight"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FocZIRPJ0fKl"
      },
      "source": [
        "## 3. Define the loss function (using the torch.nn package)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AJBoEzA-09Gs"
      },
      "outputs": [],
      "source": [
        "# In this case, we will use Cross Entropy.\n",
        "# You can either implement loss functions manually, or\n",
        "# you can use PyTorch's implementations of commonly-used loss functions.\n",
        "loss_fn = torch.nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J1k23uZJ0lM1"
      },
      "source": [
        "## 4.  Define the optimizer (using the torch.optim package)\n",
        "\n",
        "Use the optim package to define an Optimizer that will update the weights of the model for us. Here we will use SGD; the optim package contains many other optimization algorithms. The first argument to the SGD constructor tells the optimizer which Tensors it should update.\n",
        "\n",
        "In general, what distinguishes different optimizers is the way that they use gradients to define an update rule or scheme for their model weights. It follows then that any optimizer will always need the weights to be updated, and their corresponding gradients.\n",
        "\n",
        "In PyTorch, both of these quantities are stored together under model parameter variables.\n",
        "\n",
        "In particular, if `var` is a model weight, `var.grad` will contain a gradient with respect to `var` after backpropagation is performed. More in the next code cell.\n",
        "\n",
        "\n",
        "*   Bonus: One popular optimizer that is often used in practice is [Adam](https://arxiv.org/pdf/1412.6980.pdf).  Adam differs from SGD in that it adaptively updates the learning rate for each individual network weight using information about the weight's gradients throughout training.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "49YvZYnN1JPY"
      },
      "outputs": [],
      "source": [
        "learning_rate = 1e-3\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pcT2Uc1J0mp6"
      },
      "source": [
        "## 5.  Training Loop\n",
        "\n",
        "Every time you train a model, be sure to include these 3 short but important lines that do much of the heavy lifting for you!\n",
        "\n",
        "1.   `optimizer.zero_grad()`: Zeroes out gradients from previous iterations. Otherwise, over the epochs, gradients will accumulate (sum) in `parameter.grad` of each `parameter` which was passed to the optimizer as an argument.\n",
        "2.   `loss.backward()`: Does all the backpropagation in one fell swoop (takes the derivative of the loss). We call `.backward()` on the loss tensor, which has the effect of taking the partial derivative of the loss with respect to the model parameters. For a model parameter (weight) `w`, dloss/dw will be stored in `w.grad` after `loss.backward()`.\n",
        "3. `optimizer.step()`: Based on the previously specified optimizer algorithm and learning rate, updates the parameters in the direction of the gradient. This represents one iteration of the weight update rule.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tzQ7aRdY3KSW",
        "outputId": "6d310ebe-781d-40e0-a5be-056492af2c60"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "number of train set points:  50000\n"
          ]
        }
      ],
      "source": [
        "num_train_points = len(trainset)\n",
        "print('number of train set points: ', num_train_points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "po8p9RMJ4t_Q",
        "outputId": "fb54f118-fb48-41b9-a600-b322f4de97a4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "MyNeuralNetwork(\n",
              "  (hidden_layer): Linear(in_features=3072, out_features=100, bias=True)\n",
              "  (relu): ReLU()\n",
              "  (output_layer): Linear(in_features=100, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ocr3MayU6cKZ"
      },
      "outputs": [],
      "source": [
        "# Use this function to compute your model accuracy!\n",
        "# Note that the dataloader argument can take trainloader or testloader\n",
        "def compute_accuracy(model, dataloader):\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    model.train(False)\n",
        "    with torch.no_grad():\n",
        "        for data in dataloader:\n",
        "            xs, ys = data\n",
        "            sample_size = xs.shape[0]\n",
        "            xs = xs.to(device)\n",
        "            ys = ys.to(device)\n",
        "            y_preds = model(xs.reshape(sample_size, -1))\n",
        "            _, predicted = torch.max(y_preds.data, 1)\n",
        "            total += ys.size(0)\n",
        "            correct += (predicted == ys).sum().item()\n",
        "    print('Model accuracy: %d %%' % (100 * correct / total))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qj-PitLk1UWv",
        "outputId": "1b05db15-391f-4fab-8871-e5a41bf03db6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0\n",
            "Model accuracy: 11 %\n",
            "0 2.2990171909332275\n",
            "0 2.086745023727417\n",
            "1\n",
            "Model accuracy: 31 %\n",
            "1 2.0661001205444336\n",
            "1 1.9399707317352295\n",
            "2\n",
            "Model accuracy: 35 %\n",
            "2 1.758367657661438\n",
            "2 1.6997368335723877\n",
            "3\n",
            "Model accuracy: 37 %\n",
            "3 1.811711072921753\n",
            "3 1.8822017908096313\n",
            "4\n",
            "Model accuracy: 38 %\n",
            "4 1.702054738998413\n",
            "4 1.8838087320327759\n",
            "5\n",
            "Model accuracy: 39 %\n",
            "5 1.8352177143096924\n",
            "5 1.4614850282669067\n",
            "6\n",
            "Model accuracy: 41 %\n",
            "6 1.5361530780792236\n",
            "6 1.6876635551452637\n",
            "7\n",
            "Model accuracy: 42 %\n",
            "7 1.6034319400787354\n",
            "7 1.6514869928359985\n",
            "8\n",
            "Model accuracy: 42 %\n",
            "8 1.4815653562545776\n",
            "8 1.6663904190063477\n",
            "9\n",
            "Model accuracy: 42 %\n",
            "9 1.5289113521575928\n",
            "9 1.7372220754623413\n"
          ]
        }
      ],
      "source": [
        "num_epochs = 10\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  print(epoch)\n",
        "  compute_accuracy(model, testloader)\n",
        "  for datapt_idx, (x, y) in enumerate(trainloader):\n",
        "    x, y = x.to(device), y.to(device)\n",
        "\n",
        "    sample_size = x.shape[0]\n",
        "\n",
        "    # Forward pass: compute predicted y by passing x to the model.\n",
        "    y_pred = model(x.reshape(sample_size, -1))\n",
        "\n",
        "    # Compute and print loss.\n",
        "    loss = loss_fn(y_pred, y)\n",
        "\n",
        "    if datapt_idx % 1000 == 0:\n",
        "        print(epoch, loss.item())\n",
        "\n",
        "    # Before the backward pass, zero all the gradients for the variables it will update\n",
        "    # (these are the learnable weights of the model). This is because by defaults,\n",
        "    # gradients from previous iterations won't be overwritten.\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Backward pass: compute gradient of the loss with respect to model parameters\n",
        "    loss.backward()\n",
        "\n",
        "    # Calling the step function on an Optimizer makes an update to its parameters\n",
        "    optimizer.step()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z3uuSWYTSLvz"
      },
      "source": [
        "# **Exercises: Understanding PyTorch**\n",
        "\n",
        "These questions are intended to be a straightforward check on some fundamental PyTorch concepts. You are encouraged to use the PyTorch documentation and any other online resources to answer these questions.\n",
        "\n",
        "# Exercise 1\n",
        "Where are gradients stored in PyTorch and how can they be accessed?\n",
        "# Exercise 2\n",
        "In the model defined in above, how many weight parameters does the model have in each layer?\n",
        "# Exercise 3\n",
        "Conceptually, what is a fully-connected layer in a neural network, and how is it represented in PyTorch?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lOftgxoIsiZl"
      },
      "source": [
        "# Exercise 4\n",
        "**Initialize:** Modify the MyNeuralNetwork class to have 3 fully connected (linear) layers using ReLU activation functions, and with the hidden layers having 1000 nodes each.\n",
        "\n",
        "Here, we ask you to define a model class which inherits from `nn.Module` rather than using `nn.Sequential` as we saw before in [Step 2](#step2). This is done by initializing weights in `__init__()` and manually implementing the forward pass in `forward()`. Feel free to reference documentation.\n",
        "\n",
        "\n",
        "# Exercise 5\n",
        "\n",
        "**Train:** Complete the training code for the model. Use [cross-entropy loss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html) and an optimizer of your choice.\n",
        "\n",
        "Train your model for 10 epochs. Every epoch, log the train and test set loss.  Create a plot that displays both the model's train and the test set loss across epochs.\n",
        "\n",
        "\n",
        "# Exercise 6\n",
        "\n",
        "**Evaluate:** To evaluate your model, compute the following metrics on the train and test set, including them in your write-up.  For each of the following metrics, use the model's \"hard assignment\" of labels, i.e. your model's predictions should assign each data point to only 1 class:\n",
        "\n",
        "* The model's classification accuracy\n",
        "* The model's [precision and recall](https://en.wikipedia.org/wiki/Precision_and_recall) for each of the 10 classes\n",
        "\n",
        "What kinds of errors do you believe that the trained model is making?  Use evidence (such as the above metrics, example misclassified images, or any other metrics of your choice) to support your claims.\n",
        "\n",
        "# Exercise 7\n",
        "\n",
        "**Explore:** Create a new neural network with at least 1 architectural modification.   Some things you can explore are adding or removing layers, changing the number of nodes at each layer, or experimenting with convolutional and pooling layers (see Appendix). The only requirement is that your model attain at least 50% test set accuracy after training for 10 epochs.  This part of the problem is intentionally open-ended, and we encourage you to explore!\n",
        "\n",
        "For your new neural network, include a plot of train and test set loss in your writeup.  Calculate your model's train/test accuracy and precision/recall for the 10 classes.\n",
        "\n",
        "In your writeup, copy and paste your modified neural network class and describe the architectural changes you made.  Write at least 1 sentence about why you hypothesize your new neural network performed better or performed worse than the network from the tutorial.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VHkX9Z6q27-4"
      },
      "source": [
        "<a name=\"Appendix\"></a>\n",
        "# Appendix: Computation Graphs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aaUHTpkpUOHn"
      },
      "source": [
        "A *computation graph* is an abstract graphical representation of a mathematical expression involving variables. Each variable is represented as an edge, and each operation as a node; the graph is directed and acyclic. For instance, the expression $(x + y)^2 + x$ can be represented as:\n",
        "\n",
        "![Computation Graph](https://github.com/harvard-ml-courses/cs181-s21-homeworks/blob/main/hw3/T3_P3_computation_graph.jpg?raw=true)\n",
        "\n",
        "Expressions can be converted into graphs and vice versa. Computation graphs are useful representations in the context of neural networks, because\n",
        "1. They encode all the necessary information for the computation of gradients into discrete units through repeated application of the chain rule, and\n",
        "2. Graphs are easy to implement with code (conceptually, one can imagine storing pointers between nodes)\n",
        "\n",
        "The process in step #1 is known as *automatic differentiation*.\n",
        "\n",
        "If you would like to walk through a computation graph on a small neural network, we recommend [this resource](https://blog.paperspace.com/pytorch-101-understanding-graphs-and-automatic-differentiation/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ztTJRpX09z6J"
      },
      "source": [
        "# Appendix: Convolutional Neural Networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9fNNaZso91zx"
      },
      "source": [
        "A convolutional neural network (CNN) is a special type of neural network that has \"convolution layers\". CNNs are able to model spatial and temporal dependencies in ordered sequences of input data.\n",
        "\n",
        "The convolution layer of a CNN applies a filter (sometimes referred to as a \"kernel\") to an input to produce an output.\n",
        "\n",
        "Assume without loss of generality that we have two-dimensional input data $x : m \\times n$.  For example, $x$ may be the color of each pixel of a grayscale $m$ by $n$ image.\n",
        "\n",
        "During the forward pass, we slide (or \"convolve\") the filter across the input data.  In most CNNs, the filter is a function of \"regions\" of the data, where individual components of the input are grouped together.  For example, a convolutional filter applied to image data may take into account $3 x 3$ groups of pixels to model dependencies in between adjacent pixels:\n",
        "\n",
        "![CNN sliding window](https://courses.edx.org/assets/courseware/v1/7396a332c596cb606eae215d96301334/asset-v1:MITx+6.871Jx+2T2021+type@asset+block/cnn_sliding_window.gif)\n",
        "\n",
        "This GIF image shows a sliding convolutional 3 x 3 filter, being applied to 3 x 3 segments of a 4 x 4 dataset.\n",
        "\n",
        "source: https://towardsdatascience.com/intuitively-understanding-convolutions-for-deep-learning-1f6f42faee1\n",
        "\n",
        "Another example of a 1-dimensional convolution filter is illustrated by the below GIF:\n",
        "\n",
        "![Convolution operation for a simple (1 x 3) filter](https://studio.edx.org/assets/courseware/v1/20da14e015eac1863451dafe3d456e59/asset-v1:MITx+6.871Jx+2T2021+type@asset+block/cnn.gif)\n",
        "\n",
        "This animation is described below.\n",
        "\n",
        "In this example, the filter $[2, 3, 1]$ is applied as an inner product with the input vector $x = [x_1, x_2, ..., x_{10}]: 1 \\times 10$ which contains annotations of a patient EKG.  Notice here that the filter is applied to 3 successive time-steps of the input vector: the output $y_t$ at time $t$ is equal to $[2, 3, 1] \\cdot [x_t, x_{t + 1}, t_{t + 2}]$.  Therefore the output $y_t$ contains information not only about a single EKG annotation $x_t$, but also about nearby EKG annotations $x_{t + 1}$ and $x_{t + 2}$.\n",
        "\n",
        "It may be helpful to [read this article](https://adventuresinmachinelearning.com/convolutional-neural-networks-tutorial-in-pytorch/) which walks through how to create a CNN using conv and pooling layers in PyTorch."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.2"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0cb6afa0eb5f47309a71778b6fed41a4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6e8713e1ac4b4a47ac288b07416f773a",
            "max": 170498071,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b03d9640d8234968b333461d163aafe5",
            "value": 170498071
          }
        },
        "21a599bdc17a4eab9d8a6766aaabb074": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7f0005cccc024fb78835675acc781782",
              "IPY_MODEL_0cb6afa0eb5f47309a71778b6fed41a4",
              "IPY_MODEL_67b08f0df2e94faaa144bad0e928499f"
            ],
            "layout": "IPY_MODEL_603e8832de9041a7a2653de312ab70c1"
          }
        },
        "603e8832de9041a7a2653de312ab70c1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "67b08f0df2e94faaa144bad0e928499f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e9719121068246c4b57aaa7e73fec47e",
            "placeholder": "​",
            "style": "IPY_MODEL_e682e46a6b1d478caaef1660e0affcdc",
            "value": " 170498071/170498071 [00:06&lt;00:00, 29027403.90it/s]"
          }
        },
        "6e8713e1ac4b4a47ac288b07416f773a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "727a67a3733748eaa15351e5d91996d3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7f0005cccc024fb78835675acc781782": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_727a67a3733748eaa15351e5d91996d3",
            "placeholder": "​",
            "style": "IPY_MODEL_9613c3fe94f646e39f40b3bd0fc92dab",
            "value": "100%"
          }
        },
        "9613c3fe94f646e39f40b3bd0fc92dab": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b03d9640d8234968b333461d163aafe5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e682e46a6b1d478caaef1660e0affcdc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e9719121068246c4b57aaa7e73fec47e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
